# Live transcription based on OpenAI's Whisper

**Version 1.0**

## Table of Contents
- [Introduction](#introduction)
- [Requirements](#requirements)
- [How to Use](#how-to-use)
- [How it Works](#how-it-works)

## Introduction
This is a live STT (Speech-To-Text) pipeline based on the [FasterWhisper](https://github.com/SYSTRAN/faster-whisper) implementation of OpenAI's [Whisper](https://huggingface.co/openai/whisper-large-v3) model. Unlike Whisper itself, this pipeline is able to transcribe audio live and stream the results. This project is intended for things like AI-Assistants and other projects where it is beneficial to have access to an audio transcription as fast as possible.

## Requirements
This project was developed and tested on Python 3.11.7 on Windows 11 with access to an Nvidia GPU with CUDA. I cannot guarantee that it will work on other operating systems or other versions of Python. While CUDA is not required, it is highly recommended.
You will need the following libraries:

- numpy
- sounddevice
- torch
- faster_whisper
- denoiser
- langcodes

## How to Use
Create a new instance of the Transcriptor class:
```python
transcriptor = Transcriptor(microphone_index=0)
```
"microphone_index" is the only required parameter.

Call `transcriptionGenerator = transcriptor.start()` to start the transcription and receive a generator that continuously yields the current sentence spoken by the user. If a sentence is finished, it will yield this sentence, until the user starts speaking again, at which point it will yield the progress of the next sentence.

Call `transcriptor.close()` to stop the generator.

Other (optional) parameters:
- speculative (bool, default=True): Should unconfirmed results be yielded by the generator? Allows you to access the data earlier, but the results may be inaccurate and may be corrected in a subsequent pass.
- whisper_model (string, default="large-v3"): Which Whisper model to use. Smaller is faster and less resource intensive, but also less accurate.
- device (string, default="cuda" ("cpu" if "cuda" is not available)): On which device the computations should be run. CUDA is highly recommended.
- voice_boost (float, default=10): How much the audio preprocessing stage should boost the volume of the user's voice.
- language (string, default=None): Which language the Whisper model should use. Will be autodetected if None, however, this may lead to a decrease in quality. Must be a valid language code, like "en", "de", "fr".

## How it Works
The project is based on [this video](https://www.youtube.com/watch?v=_spinzpEeFM). The audio from the microphone is recorded and chunked into 1 second chunks. These chunks run through a 2 step audio preprocessing stage. In the first stage, the "denoiser" library is used to remove noise from the audio. However, the extracted voice cannot be used like this. Noise may have drowned out some of the speech. This results in audio which is low quality and won't improve the transcription results compared to the audio before the noise removal. This is why the volume of the audio without noise is boosted and it is added back to the original audio. This results in audio which, while still being noisy, has the voice of the speaker significantly increased compared to all other sounds.

In the second step of the preprocessing, the audio is passed through a VAD (Voice-Activity-Detection), specifically [this VAD](https://github.com/snakers4/silero-vad). It detects whether there is an actual voice in the audio or not. If not, the audio chunk will be used as context for later chunks, but it will not be processed further itself, as this chunk will only result in hallucinations from Whisper and a waste of processing time. However, if speech is detected, the chunk will be added to a larger chunk of audio, which represents the current sentence the user is speaking. This entire chunk will then be transcribed by Whisper. The result will be checked against the last result using a local agreement algorithm. Essentially, if a word is generated by Whisper 2 times in a row, it will be regarded as "confirmed" and cannot be changed anymore after that. This goes on until the sentence is finished and a new one is started.

The reason why this setup is so complicated is because Whisper was not trained for streaming applications, but rather it was trained to transcribe a singular sentence. This is why we always transcribe from the start of the sentence, because this will result in the best results from Whisper.

This project is licensed under the Apache License, Version 2.0. See the [LICENSE](LICENSE) file for details.

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)